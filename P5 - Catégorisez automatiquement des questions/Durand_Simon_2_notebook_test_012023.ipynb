{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f0222a",
   "metadata": {},
   "source": [
    "# <font color=\"#114b98\">Catégorisez automatiquement des questions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5815ab77",
   "metadata": {},
   "source": [
    "## <font color=\"#114b98\">Notebook de test de différents modèles</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb1151",
   "metadata": {},
   "source": [
    "**Stack Overflow** est un site célèbre de questions-réponses liées au développement informatique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d05297",
   "metadata": {},
   "source": [
    "L'objectif de ce projet est de développer un système de **suggestion de tags** pour ce site. Celui-ci prendra la forme d’un algorithme de machine learning qui assignera automatiquement plusieurs tags pertinents à une question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb25dcf2",
   "metadata": {},
   "source": [
    "**Livrable** : Un notebook de test de différents modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c6190",
   "metadata": {},
   "source": [
    "**Objectifs** : Comparer les modèles et générer des tags pour chacun d'entre eux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378bec4",
   "metadata": {},
   "source": [
    "## <font color=\"#114b98\">Sommaire</font>\n",
    "[1. Chargement du jeu de données](#section_1)\n",
    "\n",
    "[2. Approche non supervisée](#section_2)\n",
    "\n",
    "[3. Approche supervisée](#section_3)\n",
    "\n",
    "[4. Approche supervisée avec Word Embedding : Word2Vec](#section_4)\n",
    "\n",
    "[5. Approche supervisée avec Word Embedding : BERT](#section_5)\n",
    "\n",
    "[6. Approche supervisée avec Sentence Embedding : USE](#section_6)\n",
    "\n",
    "[7. Choix du modèle pour le code final à déployer](#section_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fb45a8",
   "metadata": {},
   "source": [
    "## <font color=\"#114b98\" id=\"section_1\">1. Chargement du jeu de données</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbf8947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import ast\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "376698da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b0ed41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('axes', titlesize=22) \n",
    "plt.rc('axes', labelsize=18) \n",
    "titleprops = {'fontsize':20}\n",
    "textprops = {'fontsize':15}\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b96f07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_cleaned.csv', 'data_cleaned_wo_tokenizer.csv', 'model.h5']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_path = 'N:/5 - WORK/1 - Projets/Projet 5/'\n",
    "files = os.listdir(main_path+'saved_ressources/')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a251077",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(main_path+'saved_ressources/'+'data_cleaned.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "876bc155",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.applymap(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0fd42dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>Texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[macos, emacs, scheme, interpreter, sicp]</td>\n",
       "      <td>[scheme, interpreter, interpreter, python, int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[c#, functional-programming, delegates, lambda...</td>\n",
       "      <td>[scope, bug, compiler, voidfunction, delegate,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[mysql, database, arrays, postgresql, stored-p...</td>\n",
       "      <td>[pass, array, procedure, pass, array, procedur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ios, objective-c, iphone, cocoa-touch, uibutton]</td>\n",
       "      <td>[state, buttonwithtype, figure, button, state,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[database, nhibernate, exception-handling, con...</td>\n",
       "      <td>[check, application, column, constraint, colum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tags  \\\n",
       "0          [macos, emacs, scheme, interpreter, sicp]   \n",
       "1  [c#, functional-programming, delegates, lambda...   \n",
       "2  [mysql, database, arrays, postgresql, stored-p...   \n",
       "3  [ios, objective-c, iphone, cocoa-touch, uibutton]   \n",
       "4  [database, nhibernate, exception-handling, con...   \n",
       "\n",
       "                                               Texts  \n",
       "0  [scheme, interpreter, interpreter, python, int...  \n",
       "1  [scope, bug, compiler, voidfunction, delegate,...  \n",
       "2  [pass, array, procedure, pass, array, procedur...  \n",
       "3  [state, buttonwithtype, figure, button, state,...  \n",
       "4  [check, application, column, constraint, colum...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5aa4d2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159599 entries, 0 to 159598\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   Tags    159599 non-null  object\n",
      " 1   Texts   159599 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d34fa",
   "metadata": {},
   "source": [
    "Le jeu de données est trop important pour les temps de calculs à ma disposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ffd4f",
   "metadata": {},
   "source": [
    "J'ai deux possibilités : \n",
    "- prendre un sample d'observations aléatoirement\n",
    "- prendre les observations pour lesquelles la similarité entre les deux colonnes est importante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a7568d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_sample = data.sample(20000)\n",
    "# data_sample.reset_index(inplace=True, drop=True)\n",
    "# print(data_sample.shape)\n",
    "# data_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0e79ba",
   "metadata": {},
   "source": [
    "Je choisi la seconde option afin de pouvoir regarder la pertinence des tags que mes modèles vont proposer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d247c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d1a05dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>Texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[javascript, jquery, string, date, object]</td>\n",
       "      <td>[jquery, javascript, convert, date, string, da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[listview, javafx, tableview, scrollbar, hide]</td>\n",
       "      <td>[hide, scrollbar, listview, javafx, hide, scro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[jpa, merge, entitymanager, persist, java-pers...</td>\n",
       "      <td>[jpa, entitymanager, merge, entitymanager, mer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[python, django, image, gallery, portfolio]</td>\n",
       "      <td>[create, image, gallery, create, portfolio, dj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[python, authentication, proxy, https, ntlm]</td>\n",
       "      <td>[python, https, proxy, ntlm, authentication, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tags  \\\n",
       "0         [javascript, jquery, string, date, object]   \n",
       "1     [listview, javafx, tableview, scrollbar, hide]   \n",
       "2  [jpa, merge, entitymanager, persist, java-pers...   \n",
       "3        [python, django, image, gallery, portfolio]   \n",
       "4       [python, authentication, proxy, https, ntlm]   \n",
       "\n",
       "                                               Texts  \n",
       "0  [jquery, javascript, convert, date, string, da...  \n",
       "1  [hide, scrollbar, listview, javafx, hide, scro...  \n",
       "2  [jpa, entitymanager, merge, entitymanager, mer...  \n",
       "3  [create, image, gallery, create, portfolio, dj...  \n",
       "4  [python, https, proxy, ntlm, authentication, s...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard_similarity(list1: List[str], list2: List[str]) -> float:\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    jaccard_similarity = len(intersection) / len(union)\n",
    "    return jaccard_similarity\n",
    "\n",
    "def get_highest_similarity_rows(data: pd.DataFrame, col1, col2, n):\n",
    "    data[\"jaccard_similarity\"] = data.apply(lambda x: jaccard_similarity(x[col1], x[col2]), axis=1)\n",
    "    data = data.sort_values(by=\"jaccard_similarity\", ascending=False)\n",
    "    return data.head(n)\n",
    "\n",
    "\n",
    "data_sample = get_highest_similarity_rows(data, \"Tags\", \"Texts\", 20000)\n",
    "data_sample.drop(['jaccard_similarity'], axis=1, inplace=True)\n",
    "data_sample.reset_index(inplace=True, drop=True)\n",
    "print(data_sample.shape)\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8fddddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54e6dd4",
   "metadata": {},
   "source": [
    "Je séprare le dataset obtenu en deux parties, afin de réaliser des essais surle X_train et de tester mes modèles sur le X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5761382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_sample[['Texts']]\n",
    "y = data_sample[['Tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b5a311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a96a4071",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indexes = X_train.index\n",
    "test_indexes = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "729ccef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'Texts': X_train['Texts'].tolist(), 'Tags': y_train['Tags'].tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64dcb5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texts</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[web, worker, way, worker, thread, module, sup...</td>\n",
       "      <td>[java, html, gwt, web, web-worker]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[rvm, rbenv, work, rvm, admit, knowledge]</td>\n",
       "      <td>[ruby-on-rails, ruby, rubygems, rvm, rbenv]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[javascript, javascript, application, way, jav...</td>\n",
       "      <td>[javascript, tcp, udp, websocket, packet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[crash, class, app, iphone, simulator, simulat...</td>\n",
       "      <td>[iphone, ios5, ios6, autolayout, nslayoutconst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[import, database, phpmyadmin, file, size, imp...</td>\n",
       "      <td>[mysql, database, apache, phpmyadmin, wampserver]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>[image, note, question, way, question, regex, ...</td>\n",
       "      <td>[php, .net, regex, perl, pcre]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>[advice, automate, rest, kind, rest, dept, aut...</td>\n",
       "      <td>[python, api, rest, testing, automated-tests]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>[custom, camera, view, use, camera, iphone, vi...</td>\n",
       "      <td>[ios, iphone, xcode, swift, camera]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>[expression, expression, lambda, translate, st...</td>\n",
       "      <td>[string, macros, lambda, scheme, expression]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>[list, java, value, list]</td>\n",
       "      <td>[java, swing, user-interface, documentation, u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Texts  \\\n",
       "0     [web, worker, way, worker, thread, module, sup...   \n",
       "1             [rvm, rbenv, work, rvm, admit, knowledge]   \n",
       "2     [javascript, javascript, application, way, jav...   \n",
       "3     [crash, class, app, iphone, simulator, simulat...   \n",
       "4     [import, database, phpmyadmin, file, size, imp...   \n",
       "...                                                 ...   \n",
       "9995  [image, note, question, way, question, regex, ...   \n",
       "9996  [advice, automate, rest, kind, rest, dept, aut...   \n",
       "9997  [custom, camera, view, use, camera, iphone, vi...   \n",
       "9998  [expression, expression, lambda, translate, st...   \n",
       "9999                          [list, java, value, list]   \n",
       "\n",
       "                                                   Tags  \n",
       "0                    [java, html, gwt, web, web-worker]  \n",
       "1           [ruby-on-rails, ruby, rubygems, rvm, rbenv]  \n",
       "2             [javascript, tcp, udp, websocket, packet]  \n",
       "3     [iphone, ios5, ios6, autolayout, nslayoutconst...  \n",
       "4     [mysql, database, apache, phpmyadmin, wampserver]  \n",
       "...                                                 ...  \n",
       "9995                     [php, .net, regex, perl, pcre]  \n",
       "9996      [python, api, rest, testing, automated-tests]  \n",
       "9997                [ios, iphone, xcode, swift, camera]  \n",
       "9998       [string, macros, lambda, scheme, expression]  \n",
       "9999  [java, swing, user-interface, documentation, u...  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b138a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_list = train_df[\"Texts\"].to_list()\n",
    "tags_list = train_df[\"Tags\"].to_list()\n",
    "flat_texts = [\" \".join(text) for text in texts_list]\n",
    "flat_tags = [\" \".join(tag) for tag in tags_list]\n",
    "vocabulary_texts = list(set([word for item in texts_list for word in item]))\n",
    "vocabulary_tags = list(set([word for item in tags_list for word in item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "721078cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'Texts': X_test['Texts'].tolist(), 'Tags': y_test['Tags'].tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60b05e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_list = test_df[\"Texts\"].to_list()\n",
    "test_tags_list = test_df[\"Tags\"].to_list()\n",
    "test_flat_texts = [\" \".join(text) for text in test_texts_list]\n",
    "test_flat_tags = [\" \".join(tag) for tag in test_tags_list]\n",
    "test_vocabulary_texts = list(set([word for item in texts_list for word in item]))\n",
    "test_vocabulary_tags = list(set([word for item in tags_list for word in item]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2b074",
   "metadata": {},
   "source": [
    "## <font color=\"#114b98\" id=\"section_2\">2. Approche non supervisée</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80107d0",
   "metadata": {},
   "source": [
    "### Dataset d'essais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "000d5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score, jaccard_score, precision_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import Nmf\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.matutils import corpus2dense\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8eb174",
   "metadata": {},
   "source": [
    "LDA (Latent Dirichlet Allocation) est une technique de topic modeling qui permet de découvrir les thèmes cachés (ou \"latents\") dans un ensemble de textes. Elle permet de regrouper des textes qui traitent des mêmes sujets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea29ec",
   "metadata": {},
   "source": [
    "La classe LdaModel de gensim est basée sur l'algorithme d'allocation latente de Dirichlet (LDA), qui est un modèle probabiliste génératif utilisé pour découvrir les sujets cachés dans un corpus de textes. La classe LatentDirichletAllocation de scikit-learn est également basée sur l'algorithme LDA, mais elle peut avoir des différences en termes d'implémentation, comme l'algorithme d'optimisation utilisé ou les paramètres disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ea0e17",
   "metadata": {},
   "source": [
    "NMF (Non-negative Matrix Factorization) est une autre technique de topic modeling qui permet de décomposer une matrice document-terme en deux matrices de facteurs non-négatifs. Elle est souvent utilisée pour découvrir les thèmes cachés dans des textes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96332ae",
   "metadata": {},
   "source": [
    "La classe gensim Nmf est basée sur l'algorithme de factorisation de matrice non-négative, qui est différente de la classe NMF de scikit-learn, qui est basée sur la méthode de gradient projeté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35595bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_optimal_num_topics(data, vectorizer, n_topics_range, texts_list):\n",
    "    \"\"\"\n",
    "    Given data, a vectorizer, a range of number of topics to test, and the list of texts,\n",
    "    applies the models to the data and plots the silhouette and coherence scores to help \n",
    "    determine the optimal number of topics.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Vectorize the data\n",
    "    data = vectorizer.fit_transform(data)\n",
    "    dictionary = Dictionary(texts_list)\n",
    "    corpus = [dictionary.doc2bow(txt) for txt in texts_list]\n",
    "\n",
    "    # Initialize lists to store scores for LDA and NMF\n",
    "    lda_scores = []\n",
    "    nmf_scores = []\n",
    "    coherence_nmf = []\n",
    "    coherence_lda = []\n",
    "\n",
    "    # Loop through the range of number of topics\n",
    "    for n_topics in n_topics_range:\n",
    "        \n",
    "        # Calculate the silhouette score for the LDA model\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, max_iter=1000)\n",
    "        lda.fit(data)\n",
    "        topic_assignments = lda.transform(data)\n",
    "        labels = np.argmax(topic_assignments, axis=1)\n",
    "        lda_scores.append(silhouette_score(topic_assignments, labels, metric='euclidean'))\n",
    "        \n",
    "        # Calculate the silhouette score for the NMF model\n",
    "        nmf = NMF(n_components=n_topics, max_iter=1000)\n",
    "        nmf.fit(data)\n",
    "        topic_assignments = nmf.transform(data)\n",
    "        labels = np.argmax(topic_assignments, axis=1)\n",
    "        nmf_scores.append(silhouette_score(topic_assignments, labels, metric='euclidean'))\n",
    "        \n",
    "        # Calculate the coherence score for the LDA model\n",
    "        lda = LdaModel(corpus, num_topics=n_topics, id2word=dictionary)\n",
    "        cm_lda = CoherenceModel(model=lda, texts=texts_list, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_lda.append(cm_lda.get_coherence())\n",
    "            \n",
    "        # Calculate the coherence score for the NMF model\n",
    "        nmf = Nmf(corpus, num_topics=n_topics, id2word=dictionary)\n",
    "        cm_nmf = CoherenceModel(model=nmf, texts=texts_list, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_nmf.append(cm_nmf.get_coherence())\n",
    "     \n",
    "    scores = pd.DataFrame(columns=['topics_silhouette',\n",
    "                                   'score_silhouette',\n",
    "                                   'topics_coherence',\n",
    "                                   'score_coherence'], \n",
    "                          index=['LDA', 'NMF'])\n",
    "\n",
    "    scores['topics_silhouette'] = [n_topics_range[np.argmax(lda_scores)], n_topics_range[np.argmax(nmf_scores)]]\n",
    "    scores['score_silhouette'] = [max(lda_scores), max(nmf_scores)]\n",
    "    scores['topics_coherence'] = [n_topics_range[np.argmax(coherence_lda)], n_topics_range[np.argmax(coherence_nmf)]]\n",
    "    scores['score_coherence'] = [max(coherence_lda), max(coherence_nmf)]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    plt.suptitle('Scores de Silhouette et de Coherence pour LDA et NMF avec {}'.format(str(vectorizer).split('(')[0]))\n",
    "    \n",
    "    ax1.plot(n_topics_range, lda_scores, label='LDA')\n",
    "    ax1.plot(n_topics_range, nmf_scores, label='NMF')\n",
    "    ax1.set_xlabel('Number of Topics')\n",
    "    ax1.set_ylabel('Silhouette score')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(n_topics_range, coherence_lda, label='LDA')\n",
    "    ax2.plot(n_topics_range, coherence_nmf, label='NMF')\n",
    "    ax2.set_xlabel('Number of Topics')\n",
    "    ax2.set_ylabel('Coherence score')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b21431fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of number of topics to test\n",
    "n_topics_range = range(2, 5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3537d01f",
   "metadata": {},
   "source": [
    "CountVectorizer() est une implémentation de l'approche bag-of-words pour la vectorisation de textes. Il convertit un ensemble de documents en un tableau de compte de mots (ou un sac de mots), où chaque ligne représente un document et chaque colonne représente un mot. Le nombre dans chaque cellule est le nombre de fois où le mot correspondant est présent dans le document correspondant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "898dfba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09910832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmf_and_lda_models_with_CountVectorizer = determine_optimal_num_topics(flat_texts,\n",
    "#                                                                        vectorizer,\n",
    "#                                                                        n_topics_range,\n",
    "#                                                                        texts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8734e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_and_lda_models_with_CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a118024",
   "metadata": {},
   "source": [
    "TF-IDF (term frequency-inverse document frequency) est une technique utilisée pour pondérer les termes dans les textes en fonction de leur fréquence d'apparition. Elle permet de donner plus de poids aux termes qui apparaissent fréquemment dans un document mais rarement dans l'ensemble des documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd433b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocabulary_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce7ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmf_and_lda_models_with_TfidfVectorizer= determine_optimal_num_topics(flat_texts,\n",
    "#                                                                       vectorizer,\n",
    "#                                                                       n_topics_range,\n",
    "#                                                                       texts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959aea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_and_lda_models_with_TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa9623f",
   "metadata": {},
   "source": [
    "Le score de silhouette mesure la similarité d'un objet à son propre groupe par rapport aux autres groupes et généralement, plus il est proche de 1, meilleure est la classification. Le score de cohérence mesure à quel point les sujets sont \"interprétables par les humains\", généralement plus proche de 1, meilleur c'est."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1adfb6d",
   "metadata": {},
   "source": [
    "Dans notre situation, lorsque le nombre de sujets augmente, ils davantage \"interprétables par les humains\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a1e69",
   "metadata": {},
   "source": [
    "Nous devons maintenant essayer d'obtenir des tags en utilisant ces méthodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c78d4a",
   "metadata": {},
   "source": [
    "Je choisis d'utiliser uniquement LDA pour la suite car c'est la méthode qui obtient les meilleurs scores de silhouette."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fa5a5",
   "metadata": {},
   "source": [
    "Le paramètre min_df définit le nombre minimum de documents dans lesquels un mot doit être présent pour être inclus dans le vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cdc9a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df=25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f264637",
   "metadata": {},
   "source": [
    "Le paramètre max_df définit la fréquence maximale d'un mot en pourcentage de tous les documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e8132fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d646e4df",
   "metadata": {},
   "source": [
    "Je choisis le nombre de topics au regard des résultats précédents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62dd4403",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d2a16ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags_from_text(texts_list, flat_texts, n_topics, vocabulary_texts, min_df, max_df):\n",
    "    pred_tags_gensim = list()\n",
    "    pred_tags_sklearn = list()\n",
    "    pred_tags_tfidf = list()\n",
    "    pred_tags_count = list()\n",
    "    \n",
    "    # Predict tags using LdaModel (gensim) without bow or TF-IDF \n",
    "    dictionary = Dictionary(texts_list)\n",
    "    corpus = [dictionary.doc2bow(txt) for txt in texts_list]\n",
    "    lda = LdaModel(corpus, num_topics=n_topics, id2word=dictionary, random_state=42)\n",
    "    for text in texts_list:\n",
    "        bow = dictionary.doc2bow(text)\n",
    "        topics = lda.get_document_topics(bow, minimum_probability=0)\n",
    "        topic_id, prob = max(topics, key=lambda x: x[1])\n",
    "        topic_words = [w for w, p in lda.show_topic(topic_id, topn=5)]\n",
    "        pred_tags_gensim.append(topic_words)\n",
    "\n",
    "    # Predict tags using LDA (sklearn) without bow or TF-IDF    \n",
    "    corpus_dense = corpus2dense(corpus, num_terms=len(dictionary)).T\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(corpus_dense)\n",
    "    for text in texts_list:\n",
    "        bow = dictionary.doc2bow(text)\n",
    "        dense_bow = corpus2dense([bow], num_terms=len(dictionary)).T[0]\n",
    "        dense_bow = np.reshape(dense_bow, (1, -1))\n",
    "        topic_distribution = lda.transform(dense_bow)\n",
    "        topic_id = topic_distribution.argmax()\n",
    "        top_words_indices = np.argsort(-lda.components_[topic_id])[:5]\n",
    "        topic_words = [dictionary[i] for i in top_words_indices]\n",
    "        pred_tags_sklearn.append(topic_words)     \n",
    "               \n",
    "    # Predict tags using LdaModel with TF-IDF \n",
    "    vectorizer = TfidfVectorizer(vocabulary=vocabulary_texts, min_df=min_df, max_df=max_df)\n",
    "    bow = vectorizer.fit_transform(flat_texts)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    topics = lda.fit_transform(bow)\n",
    "    for i in range(len(texts_list)):\n",
    "        topic_id = topics.argmax(axis=1)[i]\n",
    "        dense_bow_matrix = bow.toarray()\n",
    "        top_words_indices = dense_bow_matrix[i].argsort()[-5:][::-1]\n",
    "        topic_words = [list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(i)] for i in top_words_indices]\n",
    "        pred_tags_tfidf.append(topic_words)\n",
    "        \n",
    "    # Predict tags using LdaModel with CountVectorizer \n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary_texts, min_df=min_df, max_df=max_df)\n",
    "    bow = vectorizer.fit_transform(flat_texts)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    topics = lda.fit_transform(bow)\n",
    "    for i in range(len(texts_list)):\n",
    "        topic_id = topics.argmax(axis=1)[i]\n",
    "        dense_bow_matrix = bow.toarray()\n",
    "        top_words_indices = dense_bow_matrix[i].argsort()[-5:][::-1]\n",
    "        topic_words = [list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(i)] for i in top_words_indices]\n",
    "        pred_tags_count.append(topic_words)           \n",
    "        \n",
    "    return pred_tags_gensim, pred_tags_sklearn, pred_tags_tfidf, pred_tags_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f09ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags_gensim, pred_tags_sklearn, pred_tags_tfidf, pred_tags_count = get_tags_from_text(texts_list,\n",
    "                                                                                           flat_texts,\n",
    "                                                                                           n_topics,\n",
    "                                                                                           vocabulary_texts,\n",
    "                                                                                           min_df,\n",
    "                                                                                           max_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285e240",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags_gensim[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags_sklearn[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dcf1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags_tfidf[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags_count[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de06b692",
   "metadata": {},
   "source": [
    "Il semble que CountVectorizer et TfidfVectorizer prédisent des tags assez similaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9480618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_tags = tags_list\n",
    "pred_tags_list = [pred_tags_gensim, pred_tags_sklearn, pred_tags_tfidf, pred_tags_count]\n",
    "pred_names = [\"gensim\", \"sklearn\", \"tfidf\", \"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fa2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "true_tags_bin = mlb.fit_transform(true_tags)\n",
    "pred_tags_bin_list = [mlb.transform(pred_tags) for pred_tags in pred_tags_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e9a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(true_tags, pred_tags_bin_list, pred_names):\n",
    "    f1_scores = []\n",
    "    jaccard_scores = []\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    for pred_tags in pred_tags_bin_list:\n",
    "        f1_scores.append(f1_score(true_tags, pred_tags, average='samples'))\n",
    "        jaccard_scores.append(jaccard_score(true_tags, pred_tags, average='samples'))\n",
    "        accuracy_scores.append(accuracy_score(true_tags, pred_tags))\n",
    "        precision_scores.append(precision_score(true_tags, pred_tags, average='samples'))\n",
    "        \n",
    "    metrics = {\"F1\": f1_scores, \"Jaccard\": jaccard_scores, \"Accuracy\": accuracy_scores, \"Precision\": precision_scores}\n",
    "    metrics_df = pd.DataFrame(metrics, index=pred_names)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15,10))\n",
    "    axes = axes.ravel()\n",
    "    for i, metric in enumerate(metrics.keys()):\n",
    "        sns.barplot(data=metrics_df, x=metrics_df.index, y=metric, ax=axes[i])\n",
    "        axes[i].set(ylabel=metric)\n",
    "    plt.show()\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions(true_tags_bin, pred_tags_bin_list, pred_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d504bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions_one_to_one(true_tags, pred_tags_list, pred_names):\n",
    "    f1_scores = []\n",
    "    jaccard_scores = []\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    for pred_tags in pred_tags_list:\n",
    "        f1_scores.append(f1_score(true_tags, pred_tags, average='samples'))\n",
    "        jaccard_scores.append(jaccard_score(true_tags, pred_tags, average='samples'))\n",
    "        accuracy_scores.append(sum([len(set(true_tags[i]).intersection(pred_tags[i]))/len(true_tags[i]) for i in range(len(true_tags))])/len(true_tags))\n",
    "        precision_scores.append(precision_score(true_tags, pred_tags, average='samples'))\n",
    "        \n",
    "    metrics = {\"F1\": f1_scores, \"Jaccard\": jaccard_scores, \"Accuracy\": accuracy_scores, \"Precision\": precision_scores}\n",
    "    metrics_df = pd.DataFrame(metrics, index=pred_names)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15,10))\n",
    "    axes = axes.ravel()\n",
    "    for i, metric in enumerate(metrics.keys()):\n",
    "        sns.barplot(data=metrics_df, x=metrics_df.index, y=metric, ax=axes[i])\n",
    "        axes[i].set(ylabel=metric)\n",
    "    plt.show()\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions_one_to_one(true_tags_bin, pred_tags_bin_list, pred_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similar_tags(true_tags, pred_tags, method):\n",
    "    similar_counts = []\n",
    "    for pred_tags, true_tags in zip(pred_tags, true_tags):\n",
    "        similar_words = set(pred_tags) & set(true_tags)\n",
    "        similar_counts.append(len(similar_words))\n",
    "\n",
    "    counter = Counter(similar_counts)\n",
    "    counter = dict(sorted(counter.items()))\n",
    "    \n",
    "    # Add missing keys to counter with value 0\n",
    "    keys = set(range(min(counter.keys()), max(counter.keys()) + 1))\n",
    "    missing_keys = keys - set(counter.keys())\n",
    "    for key in missing_keys:\n",
    "        counter[key] = 0\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    fig.suptitle(f\"Similarité des tags avec la méthode {method}\", fontsize=18, fontweight='bold', y=1.05)\n",
    "    axs[0].bar(sorted(counter.keys()), counter.values())\n",
    "    axs[0].set_xticks(np.arange(min(counter.keys()), max(counter.keys())+1))\n",
    "    axs[0].set_xticklabels(counter.keys(), rotation=0)\n",
    "    axs[0].set_xlabel('Nombre de tags similaires', fontsize=14)\n",
    "    axs[0].set_ylabel(\"Nombre d'observations\", fontsize=14)\n",
    "    axs[0].set_title(\"Nombre d'observations avec un\\n nombre de tags similaires\", fontsize=16)   \n",
    "    axs[1].pie(counter.values(), labels=counter.keys(), autopct='%1.1f%%', pctdistance=0.8)\n",
    "    axs[1].legend(title='Tags\\nSimilaires', bbox_to_anchor=(1, 0.9), prop={'size': 8},title_fontsize=10)\n",
    "    axs[1].set_title(\"Pourcentage d'observations avec \\n un nombre de tags similaires\", fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d97840",
   "metadata": {},
   "source": [
    "### Dataset de tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a21635",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gensim, pred_sklearn, pred_tfidf, pred_count = get_tags_from_text(test_texts_list,\n",
    "                                                                       test_flat_texts,\n",
    "                                                                       n_topics,\n",
    "                                                                       test_vocabulary_texts,\n",
    "                                                                       min_df,\n",
    "                                                                       max_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099abbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similar_tags(test_tags_list, pred_gensim, 'LDA Gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3384fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similar_tags(test_tags_list, pred_sklearn, 'LDA Sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e61a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similar_tags(test_tags_list, pred_tfidf, 'LDA + TFIDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e4b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similar_tags(test_tags_list, pred_count, 'LDA + Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0747e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AJOUT PCA !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587eaddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749c530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1b4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c949a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd47d019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2325248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b65b6e3",
   "metadata": {},
   "source": [
    "## <font color=\"#114b98\" id=\"section_3\">3. Approche supervisée</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a2d53",
   "metadata": {},
   "source": [
    "### Dataset d'essais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfec925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18dac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=vocabulary_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e581924",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_mlb = mlb.fit_transform(flat_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [LogisticRegression(random_state=42, max_iter=300, tol=1e-5),\n",
    "               SGDClassifier(random_state=42, max_iter=300, tol=1e-5),\n",
    "               RandomForestClassifier(random_state=42),\n",
    "               KNeighborsClassifier(),\n",
    "               MultinomialNB()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ebb7d",
   "metadata": {},
   "source": [
    " - Accuracy: mesure de combien de prédictions faites par le modèle sont correctes\n",
    " - Precision: mesure combien des prédictions positives faites par le modèle sont effectivement correctes. Un score de précision élevé signifie que le modèle fait peu de prédictions positives fausses.\n",
    " - Recall: mesure combien des exemples positifs réels sont correctement prédits par le modèle. Un score de rappel élevé signifie que le modèle est capable de trouver la plupart des exemples positifs.\n",
    " - F1 Score: mesure de l'exactitude d'un modèle, il est un moyen harmonique de précision et de rappel. Il varie de 0 à 1, où un score proche de 1 indique une meilleure performance et un score proche de 0 indique une performance moins bonne. \n",
    " - Jaccard Score: mesure de la similarité entre les deux ensembles de prédictions et de vraies étiquettes. Il varie de 0 à 1, où un score proche de 1 indique une très grande similitude et un score proche de 0 indique une grande dissimilarité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_supervised_scores(flat_texts, tags_mlb, vectorizer, classifiers):\n",
    "    \n",
    "    # Create an empty dataframe to store the results\n",
    "    results_df = pd.DataFrame(columns=['Classifier', 'Accuracy', 'Precision', 'Recall',\n",
    "                                       'F1 Score', 'Jaccard Score', 'Time (s)'])\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(flat_texts, tags_mlb, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Vectorize X_train and X_test\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Fit an independent model for each class using the OneVsRestClassifier wrapper.\n",
    "    for classifier in classifiers:\n",
    "        start_time = time.time()\n",
    "        ovrc = OneVsRestClassifier(classifier)\n",
    "        ovrc.fit(X_train, y_train)\n",
    "        y_pred_ovrc = ovrc.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        accuracy = round(accuracy_score(y_test, y_pred_ovrc), 4)\n",
    "        precision = round(precision_score(y_test, y_pred_ovrc, average='samples'), 4)\n",
    "        recall = round(recall_score(y_test, y_pred_ovrc, average='samples'), 4)\n",
    "        f1 = round(f1_score(y_test, y_pred_ovrc, average='samples'), 4)\n",
    "        jaccard = round(jaccard_score(y_test, y_pred_ovrc, average='samples'), 4)\n",
    "        time_taken = round(end_time - start_time, 4)\n",
    "        \n",
    "        results_df = results_df.append({'Classifier': str(classifier).split('(')[0], \n",
    "                                       'Accuracy': accuracy, \n",
    "                                       'Precision': precision, \n",
    "                                       'Recall': recall, \n",
    "                                       'F1 Score': f1, \n",
    "                                       'Jaccard Score': jaccard,\n",
    "                                       'Time (s)': time_taken}, \n",
    "                                       ignore_index=True)\n",
    "                                       \n",
    "        print('Results for classifier:', classifier)\n",
    "        print(\"Accuracy : \", accuracy)\n",
    "        print(\"Precision : \", precision)\n",
    "        print(\"Recall : \", recall)\n",
    "        print(\"F1 Score : \", f1)\n",
    "        print(\"Jaccard Score:\", jaccard)\n",
    "        print(\"Time (s):\", time_taken)\n",
    "        print('\\n')\n",
    "        \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5fbbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b6b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_CountVectorizer = calculate_supervised_scores(flat_texts,\n",
    "                                                         tags_mlb,\n",
    "                                                         vectorizer,\n",
    "                                                         classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa632ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec7ddd9",
   "metadata": {},
   "source": [
    "Selon ces résultats, il semble que le RandomForestClassifier a les meilleures performances globales, avec la plus grande précision, rappel et score F1. Le classificateur de régression logistique se comporte également bien, avec une précision légèrement inférieure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324f513",
   "metadata": {},
   "source": [
    "Cependant, le RandomForestClassifier est aussi le seul à nécessiter un temps d'entraînement aussi long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results_df):\n",
    "    \n",
    "    # Create a figure with 5 subplots\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(20,10))\n",
    "    # Set a color palette\n",
    "    my_palette = sns.color_palette(\"husl\", 5)\n",
    "    \n",
    "    # Set the x-axis to be a range of numerical values\n",
    "    x = range(len(results_df))\n",
    "    scoring_methods = ['Accuracy','Precision','Recall','F1 Score','Jaccard Score', 'Time (s)']\n",
    "    \n",
    "    # Create a subplot for each scoring method\n",
    "    for i, scoring_method in enumerate(scoring_methods):\n",
    "        sns.barplot(x='Classifier', \n",
    "                    y=scoring_method, \n",
    "                    data=results_df, \n",
    "                    ax=axs[i // 3, i % 3], \n",
    "                    palette=my_palette, \n",
    "                    label=scoring_method)\n",
    "    \n",
    "    # Add classifier names to x-axis\n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "            axs[i,j].set_title(scoring_methods[i*3+j])\n",
    "            axs[i,j].set_xticks(x)\n",
    "            axs[i,j].set_xlabel('')\n",
    "            \n",
    "            if j == 0:\n",
    "                axs[i,j].set_ylabel('Score')\n",
    "            else:\n",
    "                axs[i,j].set_ylabel('')\n",
    "            \n",
    "            if i == 1:\n",
    "                axs[i,j].set_xticklabels(results_df['Classifier'], rotation=90)\n",
    "            else:\n",
    "                axs[i,j].set_xticklabels('')\n",
    "            \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb1797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the plot_results function\n",
    "plot_results(results_df_CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocabulary_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183b758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_TfidfVectorizer = calculate_supervised_scores(flat_texts,\n",
    "                                                         tags_mlb,\n",
    "                                                         vectorizer,\n",
    "                                                         classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the plot_results function\n",
    "plot_results(results_df_TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de6aede",
   "metadata": {},
   "source": [
    "Le RandomForestClassifier obtient les meilleurs scores, peu importe le vectorizer utilisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cdc2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_RFC = pd.DataFrame(columns=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Jaccard Score'])\n",
    "results_RFC = results_RFC.append(results_df_CountVectorizer.iloc[2,1:]).reset_index(drop=True)\n",
    "results_RFC = results_RFC.append(results_df_TfidfVectorizer.iloc[2,1:]).reset_index(drop=True)\n",
    "results_RFC['Classifier'] = ['CountVectorizer', 'TfidfVectorizer']\n",
    "results_RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the plot_results function\n",
    "plot_results(results_RFC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f267676",
   "metadata": {},
   "source": [
    "CountVectorizer permet d'avoir des performances similaires avec un temps d'entraînement plus court que TfidfVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093da37",
   "metadata": {},
   "source": [
    "### Dataset de tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bbff03",
   "metadata": {},
   "source": [
    "## <font color=\"#114b98\" id=\"section_4\">4. Approche supervisée avec Word Embedding : Word2Vec</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e96c0",
   "metadata": {},
   "source": [
    "### Dataset d'essais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc479b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts_list,\n",
    "                                                    tags_mlb,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0b0888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model on your text data\n",
    "w2v_model = Word2Vec(X_train, vector_size=1000, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6252ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary of only the words in the text data that are in the word2vec model\n",
    "vocab = set(w2v_model.wv.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b3572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the text data to only include words in the vocabulary\n",
    "X_train = [[word for word in sublist if word in vocab] for sublist in X_train]\n",
    "X_test = [[word for word in sublist if word in vocab] for sublist in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2097c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any observations that have no words in the vocabulary\n",
    "train_removed_indexes = []\n",
    "test_removed_indexes = []\n",
    "for i, sublist in enumerate(X_train):\n",
    "    if not any(word in vocab for word in sublist):\n",
    "        train_removed_indexes.append(i)\n",
    "for i, sublist in enumerate(X_test):\n",
    "    if not any(word in vocab for word in sublist):\n",
    "        test_removed_indexes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde4e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [x for i, x in enumerate(X_train) if i not in train_removed_indexes]\n",
    "X_test = [x for i, x in enumerate(X_test) if i not in test_removed_indexes]\n",
    "y_train = [x for i, x in enumerate(y_train) if i not in train_removed_indexes]\n",
    "y_test = [x for i, x in enumerate(y_test) if i not in test_removed_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f1f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for train and test data\n",
    "X_train_embedded = [np.mean([w2v_model.wv[word] for word in sentence], axis=0) for sentence in X_train]\n",
    "X_test_embedded = [np.mean([w2v_model.wv[word] for word in sentence], axis=0) for sentence in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433200ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(LogisticRegression(random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the classifier on the train data\n",
    "clf.fit(X_train_embedded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23460475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scores\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='micro')\n",
    "recall = recall_score(y_test, y_pred, average='micro')\n",
    "f1 = f1_score(y_test, y_pred, average='micro')\n",
    "jaccard = jaccard_score(y_test, y_pred, average='samples')\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1-Score: {:.2f}\".format(f1))\n",
    "print(\"Jaccard Score: {:.2f}\".format(jaccard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [LogisticRegression(random_state=42, max_iter=300, tol=1e-5),\n",
    "               SGDClassifier(random_state=42, max_iter=300, tol=1e-5),\n",
    "               RandomForestClassifier(random_state=42),\n",
    "               KNeighborsClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f849d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_supervised_word2vec(X_train, X_test, y_train, y_test, classifiers):\n",
    "    \n",
    "    # Create an empty dataframe to store the results\n",
    "    results_df = pd.DataFrame(columns=['Classifier', 'Accuracy', 'Precision', 'Recall',\n",
    "                                       'F1 Score', 'Jaccard Score', 'Time (s)'])\n",
    "    \n",
    "    # Fit an independent model for each class using the OneVsRestClassifier wrapper.\n",
    "    for clf in classifiers:\n",
    "        start_time = time.time()\n",
    "        ovrc = OneVsRestClassifier(clf)\n",
    "        ovrc.fit(X_train, y_train)\n",
    "        y_pred_ovrc = ovrc.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        accuracy = round(accuracy_score(y_test, y_pred_ovrc), 4)\n",
    "        precision = round(precision_score(y_test, y_pred_ovrc, average='samples'), 4)\n",
    "        recall = round(recall_score(y_test, y_pred_ovrc, average='samples'), 4)\n",
    "        f1 = round(f1_score(y_test, y_pred_ovrc, average='samples'), 4)\n",
    "        jaccard = round(jaccard_score(y_test, y_pred_ovrc, average='samples'), 4)\n",
    "        time_taken = round(end_time - start_time, 4)\n",
    "        \n",
    "        results_df = results_df.append({'Classifier': str(clf).split('(')[0], \n",
    "                                       'Accuracy': accuracy, \n",
    "                                       'Precision': precision, \n",
    "                                       'Recall': recall, \n",
    "                                       'F1 Score': f1, \n",
    "                                       'Jaccard Score': jaccard,\n",
    "                                       'Time (s)': time_taken}, \n",
    "                                       ignore_index=True)\n",
    "        \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256504e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_word2vec = calculate_supervised_word2vec(X_train_embedded,\n",
    "                                                    X_test_embedded,\n",
    "                                                    y_train,\n",
    "                                                    y_test,\n",
    "                                                    classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc654154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the plot_results function\n",
    "plot_results(results_df_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3710df9",
   "metadata": {},
   "source": [
    "### Dataset de tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ccdd5b",
   "metadata": {},
   "source": [
    "## <font color=\"#114b98\" id=\"section_5\">5. Approche supervisée avec Word Embedding : BERT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a28454b",
   "metadata": {},
   "source": [
    "### Dataset d'essais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d8f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import tokenization\n",
    "from transformers import BertTokenizer, AutoModel, BertTokenizerFast\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from torch import nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d40ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_tags = set([word for word, count in Counter([word for item in tags_list for word in item]).most_common() if count >= 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fa741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install  torch===1.7.1+cu110 torchvision===0.8.2+cu110 torchaudio===0.7.2 --user -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54798c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data_sample['Texts'].copy()\n",
    "tags = data_sample['Tags'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texts)):\n",
    "    texts[i] = \" \".join(texts[i])\n",
    "    tags[i] = \" \".join(tags[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23821031",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "tags_bin = mlb.fit_transform(tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23dcae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text, train_labels, test_labels = train_test_split(texts, \n",
    "                                                                    tags_bin, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b096e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d71f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3da1faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9404e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = bert_encode(train_text.values, tokenizer, max_len=100)\n",
    "tokens_test = bert_encode(test_text.values, tokenizer, max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f174f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    out = Dense(len(train_labels[0]), activation='sigmoid')(clf_output)    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e45cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(bert_layer, max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94235bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a866b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea0ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use one-hot encoded labels for training\n",
    "train_history = model.fit(\n",
    "    tokens_train, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a3d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.lineplot(x=np.arange(1,11), y=train_history.history['loss'], label=\"Training Loss\", ax=axs[0])\n",
    "sns.lineplot(x=np.arange(1,11), y=train_history.history['val_loss'], label=\"Validation Loss\", ax=axs[0])\n",
    "axs[0].set_title(\"Loss\")\n",
    "axs[0].set_xticks(np.arange(1,11))\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "\n",
    "sns.lineplot(x=np.arange(1,11), y=train_history.history['binary_accuracy'], label=\"Training Accuracy\", ax=axs[1])\n",
    "sns.lineplot(x=np.arange(1,11), y=train_history.history['val_binary_accuracy'], label=\"Validation Accuracy\", ax=axs[1])\n",
    "axs[1].set_title(\"Accuracy\")\n",
    "axs[1].set_xticks(np.arange(1,11))\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f54cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(tokens_test, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for text in tqdm(flat_texts):\n",
    "    preds = model.predict(bert_encode(text, tokenizer, max_len=100))\n",
    "    indices = np.argsort(preds)[0][-5:]\n",
    "    preds[0, indices] = 1\n",
    "    preds[np.where(preds != 1)] = 0\n",
    "    decoded = mlb.inverse_transform(preds)\n",
    "    predictions.append(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af59b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tags(row):\n",
    "    tags = row.split()\n",
    "    tags = [f\"'{tag}'\" for tag in tags]\n",
    "    return tuple(tags)\n",
    "\n",
    "converted_tags = tags.apply(convert_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4800d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3dbb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_true_tags = [ast.literal_eval(tag[1:-1]) for tag in tags.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cea21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_true_tags[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47f1792",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similar_tags(converted_tags, predictions, 'BERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d1987",
   "metadata": {},
   "source": [
    "### Dataset de tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddef9c4",
   "metadata": {},
   "source": [
    "## <font color=\"#114b98\" id=\"section_6\">6. Approche supervisée avec Sentence Embedding : USE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137809e",
   "metadata": {},
   "source": [
    "### Dataset d'essais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "# from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac780d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_use = pd.read_csv(main_path+'saved_ressources/'+'data_cleaned_wo_tokenizer.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data_use['Texts'].to_list()\n",
    "tags = data_use['Tags'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df00ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_sentences = [sentences[i] for i in train_indexes]\n",
    "extracted_tags = [tags[i] for i in train_indexes]\n",
    "parsed_true_tags = [ast.literal_eval(tags[1:-1]) for tags in extracted_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(input_text):\n",
    "    # Load the USE model\n",
    "    model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "    all_keywords = []\n",
    "    # Encode all the sentences in the input text\n",
    "    embeddings = model(input_text)\n",
    "    # Compute the cosine similarity between all the sentences\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    # Iterate over the list of sentences\n",
    "    for i in range(len(input_text)):\n",
    "        # Find the most similar sentences\n",
    "        most_similar = np.argsort(-similarity_matrix[i])[1:6]\n",
    "        # Combine the most similar sentences with the current sentence\n",
    "        text = ' '.join([input_text[j] for j in most_similar])\n",
    "        # Extract the keywords from the combined text using RAKE\n",
    "        keyword_extractor = Rake()\n",
    "        keyword_extractor.extract_keywords_from_text(text)\n",
    "        word_degrees = keyword_extractor.get_word_degrees()\n",
    "        sorted_word_degrees = sorted(word_degrees.items(), key=lambda x: x[1], reverse=True)\n",
    "        keywords = [word for word, degree in sorted_word_degrees[:5]]\n",
    "        all_keywords.append(keywords)\n",
    "    return all_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = extract_keywords(extracted_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055932cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbd3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_true_converted_tags = [list(tag) for tag in parsed_true_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_true_converted_tags[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beef28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similar_tags(parsed_true_converted_tags, keywords, 'USE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972fa1d9",
   "metadata": {},
   "source": [
    "### Dataset de tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da9841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_extracted_sentences = [sentences[i] for i in test_indexes]\n",
    "test_extracted_tags = [tags[i] for i in test_indexes]\n",
    "test_parsed_true_tags = [ast.literal_eval(tags[1:-1]) for tags in test_extracted_tags]\n",
    "test_keywords = extract_keywords(test_extracted_sentences)\n",
    "test_parsed_true_converted_tags = [list(tag) for tag in test_parsed_true_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b4e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_keywords[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a802b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parsed_true_converted_tags[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82425db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similar_tags(test_parsed_true_converted_tags, test_keywords, 'USE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a190d",
   "metadata": {},
   "source": [
    "## <font color=\"#114b98\" id=\"section_7\">7. Choix du modèle pour le code final à déployer</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d215bd4",
   "metadata": {},
   "source": [
    "Nous allons maintenant comparer les différents modèles à l'aide de leurs résultats sur le dataset de tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf14c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589e1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a91f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6979e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b2567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa35568c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643ec06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca872b56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
