{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086e1c45",
   "metadata": {},
   "source": [
    "# <font color=\"#114b98\">Catégorisez automatiquement des questions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a5335",
   "metadata": {},
   "source": [
    "## <font color=\"#114b98\">Code final à déployer</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa35d506",
   "metadata": {},
   "source": [
    "**Stack Overflow** est un site célèbre de questions-réponses liées au développement informatique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f53d9",
   "metadata": {},
   "source": [
    "L'objectif de ce projet est de développer un système de suggestion de tags pour ce site. Celui-ci prendra la forme d’un algorithme de machine learning qui assignera automatiquement plusieurs tags pertinents à une question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021875eb",
   "metadata": {},
   "source": [
    "**Livrable** : Le code final à déployer présenté dans un répertoire et développé progressivement à l’aide d’un logiciel de gestion de versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e70c7e",
   "metadata": {},
   "source": [
    "### Code final :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0099f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x2620b437b50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import joblib\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf5f72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des balises HTML\n",
    "def clean_html(text):\n",
    "    soup = BeautifulSoup(text, \"html5lib\")\n",
    "    for sent in soup(['style', 'script']):\n",
    "        sent.decompose()\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "\n",
    "\n",
    "# Nettoyage du texte\n",
    "def clean_text(text):\n",
    "    pattern = re.compile(r'[^\\w]|[\\d_]')\n",
    "    res = re.sub(pattern, \" \", text)\n",
    "    res = \" \".join(word for word in res.split() if len(word) >= 3)\n",
    "    return res\n",
    "\n",
    "\n",
    "# Tokenisation et retrait des stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "gensim_stopwords = set(STOPWORDS)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_stopwords = set(nlp.Defaults.stop_words)\n",
    "stop_words = nltk_stopwords.union(gensim_stopwords, spacy_stopwords)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "\n",
    "# POS Tagging\n",
    "def filtering_nouns(tokens):\n",
    "    res = [token.lower() for token, tag in pos_tag(tokens) if tag == 'NN']\n",
    "    return res\n",
    "\n",
    "\n",
    "# Lemmatisation\n",
    "def lemmatization(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    txt = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return [\" \".join(txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e48ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "post = 'Tensorflow 2.0 - AttributeError: module tensorflow has no attribute Session\\\n",
    "        When I am executing the command sess = tf.Session() \\\n",
    "        in Tensorflow 2.0 environment, I am getting an error message as below:\\\n",
    "        Traceback (most recent call last):\\\n",
    "        File \"<stdin>\", line 1, in <module>\\\n",
    "        AttributeError: module tensorflow has no attribute Session\\\n",
    "        System Information:\\\n",
    "        OS Platform and Distribution: Windows 10\\\n",
    "        Python Version: 3.7.1\\\n",
    "        Tensorflow Version: 2.0.0-alpha0 (installed with pip)\\\n",
    "        Steps to reproduce:\\\n",
    "        Installation:\\\n",
    "        pip install --upgrade pip\\\n",
    "        pip install tensorflow==2.0.0-alpha0\\\n",
    "        pip install keras\\\n",
    "        pip install numpy==1.16.2\\\n",
    "        Execution:\\\n",
    "        Execute command: import tensorflow as tf\\\n",
    "        Execute command: sess = tf.Session()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce7a84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the sentence\n",
    "txt = lemmatization(filtering_nouns(tokenize(clean_text(clean_html(post)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db79e220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['module attribute session command sess environment message line module attribute alpha reproduce pip install pip pip install alpha pip install command import command sess session']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69532998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved CountVectorizer\n",
    "vectorizer_loaded = joblib.load('countvectorizer.joblib')\n",
    "\n",
    "# load the saved classifier\n",
    "clf_loaded = joblib.load('sgdc_classifier.pkl')\n",
    "\n",
    "# load the saved MultiLabelBinarizer\n",
    "mlb_loaded = joblib.load('multilabelbinarizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f6e21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate tags\n",
    "txt_vect = vectorizer_loaded.transform(txt)\n",
    "tags_mlb = clf_loaded.predict(txt_vect)\n",
    "tags = mlb_loaded.inverse_transform(tags_mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ff9e481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python', 'python-3.x', 'pip', 'conda', 'session')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91432f5b",
   "metadata": {},
   "source": [
    "le POS_TAGGING a bloqué le keyword tensorflow pourtant important ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65dc722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563c56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29474c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad1a4b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_with_vectorizer(x):\n",
    "    return vectorizer_loaded.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f74acd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('html_cleaner', FunctionTransformer(clean_html)),\n",
    "    ('text_cleaner', FunctionTransformer(clean_text)),\n",
    "    ('tokenizer', FunctionTransformer(tokenize)),\n",
    "    ('noun_filter', FunctionTransformer(filtering_nouns)),\n",
    "    ('lemmatizer', FunctionTransformer(lemmatization)),\n",
    "    ('vectorizer', FunctionTransformer(transform_with_vectorizer)),\n",
    "    ('clf', clf_loaded)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba7116b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('python', 'python-3.x', 'pip', 'conda', 'session')]\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model and transform the text\n",
    "tags = mlb_loaded.inverse_transform(text_clf.predict(post))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b544b98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bca333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
